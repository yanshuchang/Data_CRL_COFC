lr-trpo

More details, please refer to https://arxiv.org/abs/1502.05477 (TRPO) and
https://arxiv.org/abs/2007.03964 (PID Lagrangian).


float cost_limit:                 The constraint limit(s) for the Lagrangian optimization (1.50).
str device:                       The device to use for training and inference, "cpu".
int thread:                       The number of threads to use for training, ignored if `device` is "cuda", 4.
int seed:                         The random seed for reproducibility, 10.
float lr:                         The learning rate, 5e-4.
float target_kl:                  The target KL divergence for the line search (0.001).
Tuple[int, ...] hidden_sizes:     The sizes of the hidden layers for the policy and value networks, (128, 128).
bool unbounded:                   Whether the action space is unbounded, False.
bool last_layer_scale:            Whether to scale the last layer output for the policy network, False.
float backtrack_coeff:            The coefficient for backtracking during the line search (0.8).
int max_backtracks:               The maximum number of backtracks allowed during the line search (10).
int optim_critic_iters:           The number of optimization iterations for the critic network (20).
float gae_lambda:                 The GAE lambda value (0.95).
bool advantage_normalization:     Whether to normalize advantage (True).
bool use_lagrangian:              Whether to use the Lagrangian constraint optimization (True).
bool rescaling:                   Whether use the rescaling trick for Lagrangian multiplier, see
        Alg. 1 in http://proceedings.mlr.press/v119/stooke20a/stooke20a.pdf
float gamma:                      The discount factor for future rewards (0.98).
int max_batchsize:                The maximum size of the batch when computing GAE, depends on the size of available memory and the memory cost of the model; should be as
                                  large as possible within the memory constraint. Default to 99999.
bool reward_normalization:        Whether to normalize rewards (False).
bool deterministic_eval:          Whether to use deterministic action selection during evaluation (True).
bool action_scaling:              Whether to scale the actions according to the action space bounds (True).
str action_bound_method:          The method for handling actions that exceed the action space bounds ("clip" or other custom methods) ("clip").
Optional[torch.optim.lr_scheduler.LambdaLR] lr_scheduler: Learning rate scheduler for the optimizer (None).
